---
title: "data_wrangling_2"
author: "Jana Lee"
date: "10/10/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(rvest)
library(httr)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Lecture: Reading Data from the Web
**2 ways to get data:**
  1) Data included as content on a webpage, what you want to "scrape"
  2) Dedicated server holding data in a relatively usable form (data.gov)
Styling = CSS, makes stuff pretty

**Application Programming Interfaces (APIs)** - someone has set up a way to let you access data. Don't need to extract HTML

**Real talk about web data** - 
- data from web is messy
- it will frequently take a lot of work to figure out

## Extracting Tables - Reading in NSDH Data
Example is from the National Survey on Drug Use and Health. Includes tables relating to drug use in the past year or month, separately for specific kinds of drug use.
```{r}
url = "http://samhda.s3-us-gov-west-1.amazonaws.com/s3fs-public/field-uploads/2k15StateFiles/NSDUHsaeShortTermCHG2015.htm"
drug_use_xml = 
  read_html(url) 

drug_use_xml %>% 
  html_nodes(css = "table")
# We can see that this is not too helpful, so we need to pull out HTML nodes. When using `html_nodes`, we see that there are 15 tables produced.

# Use [[]] double brackets to 

table_list = (drug_use_xml %>%  html_nodes(css = "table"))
table_list [[1]] %>% 
  html_table() %>% 
  slice(-1)

# Table structure ALMOST works, but the first element is this note. The footnote is being extracted as a single data element. Need to take this piece out by slide function

table_marj =
  table_list [[1]] %>% 
  html_table() %>% 
  slice(-1) %>% 
  as_tibble()
```

## Learning Assessment
```{r}

```


## CSS Selectors
Getting HP data
```{r}
hpsaga_html = 
  read_html("https://www.imdb.com/list/ls000630791/")
```

Need to get HP data nodes:
```{r}
hp_movie_name = 
  hpsaga_html %>% 
  html_nodes(".lister-item-header a") %>% 
  html_text

hp_runtime = 
  hpsaga_html %>% 
  html_nodes(".runtime") %>% 
  html_text
# Get "lister" from the Selector Gadget!! It knows!!!
```

## Learning Assessment: Getting Napolean Dynamite Review Data
```{r}
url = "https://www.amazon.com/product-reviews/B00005JNBQ/ref=cm_cr_arp_d_viewopt_rvwer?ie=UTF8&reviewerType=avp_only_reviews&sortBy=recent&pageNumber=1"

dynamite_html = read_html(url)

review_titles = 
  dynamite_html %>%
  html_nodes(".a-text-bold span") %>%
  html_text()

review_stars = 
  dynamite_html %>%
  html_nodes("#cm_cr-review_list .review-rating") %>%
  html_text()

review_text = 
  dynamite_html %>%
  html_nodes(".review-text-content span") %>%
  html_text()

reviews = tibble(
  title = review_titles,
  stars = review_stars,
  text = review_text
)
# Tibble creates a DF.
```




## Using an API w/ Water Consumption in NYC
```{r}
nyc_water = 
  GET("https://data.cityofnewyork.us/resource/ia2d-e54m.csv") %>% 
  content("parsed")

# Get = helps us get a URL. Can be JSON or CSV (CSV might be easier?)
# Content = Extract content from a request
```

More examples for API Extraction
```{r}
poke =
  GET("http://pokeapi.co/api/v2/pokemon/1") %>%
  content()
```





# Lecture: Strings vs. Factors
Both look like character vectors, but strings are just like strings. Factors have an underlying numeric structure. Stringr package is what we will be using!

Regular expressions = naming things in r, what you have named, you might need to search for them.

Factors = few categorical variables siting on top of each other. Forcats package.
-when you use linear models, need reference categories
- have to be in control of factor variables
-for categorical variables and anagram for factors

##Strings and Regex
```{r}
library(p8105.datasets)

string_vec = c("my", "name", "is", "jeff")

str_detect(string_vec, "jeff")
#str prefix: every function that we want to use in stringr package is this. We can scroll through str to see what exists. str_detect detects whether "jeff" is in each of the strings. It doesn't exist in the first 3, but it does in the last.
```

str_replace
```{r}
str_replace(string_vec, "jeff", "Jeff")
```

New string vector
```{r}
string_vec = c(
  "i think we all rule for participating",
  "i think i have been caught",
  "i think this will be quite fun actually",
  "it will be fun, i think"
  )
str_detect(string_vec, "^i think")
# ^ is start of a string variable

str_detect(string_vec, "^i think$")
# $ends with a string variable
```

New string vector
```{r}
string_vec = c(
  "Y'all remember Pres. HW Bush?",
  "I saw a green bush",
  "BBQ and Bushwalking at Molonglo Gorge",
  "BUSH -- LIVE IN CONCERT!!"
  )
str_detect(string_vec, "[Bb]ush")
# detecting any match of "bush". Either start with a capital B or lowercase b. Doesn't detect all caps!
```

New string vector
```{r}
string_vec = c(
  '7th inning stretch',
  '1st half soon to begin. Texas won the toss.',
  'she is 5 feet 4 inches tall',
  '3AM - cant sleep :('
  )
str_detect(string_vec, "[0-9][a-zA-Z]")
# any number from 0-9, followed by any letter lower case a-z to upper case a-z
```

New string vector
```{r}
string_vec = c(
  'Its 7:11 in the evening',
  'want to go to 7-11?',
  'my flight is AA711',
  'NetBios: scanning ip 203.167.114.66'
  )
str_detect(string_vec, "7.11")
# finds 7 number followed by 11. . matches any character at all. AA711 doesn't count bc no character in between 7 and
```










